# Classification-model-for-MNIST

A Convolutional Neural Network (CNN) built in PyTorch to classify handwritten digits from 0 to 9 using the **MNIST** dataset. The model achieves high accuracy by using a 3-layer CNN with dropout, normalization, and proper regularization techniques.

---

## ğŸ“Œ Project Overview

- **Dataset**: MNIST (from Keras)
- **Model**: 3-layer CNN
- **Framework**: PyTorch
- **Goal**: Multiclass classification (10 classes: digits 0â€“9)
- **Accuracy**: Achieves >99% test accuracy in 5 epochs

---

## ğŸ“ Dataset: MNIST

The MNIST dataset consists of:
- ğŸ–¼ï¸ 60,000 training images
- ğŸ§ª 10,000 test images
- Grayscale digits (28x28 pixels)

Data is loaded using:
```python
from keras.datasets import mnist
```

---

## ğŸ§  Model Architecture

```text
Input: (1 x 28 x 28)
â”‚
â”œâ”€â”€ Conv2D(1 â†’ 32, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Conv2D(32 â†’ 64, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Conv2D(64 â†’ 128, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Flatten (128 * 3 * 3)
â”œâ”€â”€ Dense(256) + ReLU + Dropout(0.5)
â””â”€â”€ Dense(10) â†’ Softmax (via CrossEntropyLoss)
```

---

## âš™ï¸ Training Configuration

| Setting           | Value               |
|------------------|---------------------|
| Optimizer        | Adam                |
| Loss Function    | CrossEntropyLoss     |
| Learning Rate    | 0.0005              |
| Weight Decay     | 1e-4                |
| Batch Size       | 128                 |
| Epochs           | 10 (configurable)   |
| Scheduler        | StepLR (gamma=0.5)  |

Training and test accuracy are evaluated per epoch.

---

## ğŸ“‰ Visualizations

### ğŸ”º Accuracy Plot
- Compares **training vs. test accuracy** across epochs.

### ğŸ“‰ Smoothed Loss Curve
- Shows **loss vs. iterations** with smoothing using moving average.
- Log-scaled Y-axis to visualize exponential drop.

<img src="loss_plot_example.png" alt="Loss vs. Iterations" width="600">

---

## ğŸš€ How to Run

1. **Clone this repo**
   ```bash
   git clone https://github.com/your-username/mnist-cnn-classifier.git
   cd mnist-cnn-classifier
   ```

2. **Install dependencies**
   ```bash
   pip install torch torchvision scikit-learn matplotlib keras
   ```

3. **Run the training script**
   ```bash
   python train.py
   ```

4. **Output**
   - Accuracy plots
   - Loss curve
   - Sample predictions

---

## ğŸ“¦ Sample Prediction Output

10 random test samples with model predictions:

```
True: 7 | Predicted: 7
True: 2 | Predicted: 2
True: 1 | Predicted: 1
...
```

![Sample Predictions](sample_predictions.png)

---

## ğŸ“š Learnings & Takeaways

- Mini-batch training can be noisy; use smoothing for clarity.
- Log scaling helps visualize exponential loss decay.
- Dropout helps reduce overfitting significantly in this task.

---

## ğŸ Future Improvements

- Add early stopping
- Add validation split
- Visualize confusion matrix
- Export model with `torch.save()`

---

## ğŸ‘¨â€ğŸ’» Author

- Name: *Your Name*
- GitHub: [@your-username](https://github.com/your-username)
- LinkedIn / Blog / Portfolio (optional)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
