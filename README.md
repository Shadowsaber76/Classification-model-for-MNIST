# Classification-model-for-MNIST

A Convolutional Neural Network (CNN) built in PyTorch to classify handwritten digits from 0 to 9 using the **MNIST** dataset. The model achieves high accuracy by using a 3-layer CNN with dropout, normalization, and proper regularization techniques.

---

## ğŸ“Œ Project Overview

- **Dataset**: MNIST (from Keras)
- **Model**: 3-layer CNN
- **Framework**: PyTorch
- **Goal**: Multiclass classification (10 classes: digits 0â€“9)
- **Accuracy**: Achieves >99% test accuracy in 5 epochs

---

## ğŸ“ Dataset: MNIST

The MNIST dataset consists of:
- ğŸ–¼ï¸ 60,000 training images
- ğŸ§ª 10,000 test images
- Grayscale digits (28x28 pixels)

Data is loaded using:
```python
from keras.datasets import mnist
```

---

## ğŸ§  Model Architecture

```text
Input: (1 x 28 x 28)
â”‚
â”œâ”€â”€ Conv2D(1 â†’ 32, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Conv2D(32 â†’ 64, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Conv2D(64 â†’ 128, 3x3) + ReLU
â”œâ”€â”€ MaxPool(2x2)
â”‚
â”œâ”€â”€ Flatten (128 * 3 * 3)
â”œâ”€â”€ Dense(256) + ReLU + Dropout(0.5)
â””â”€â”€ Dense(10) â†’ Softmax (via CrossEntropyLoss)
```

---

## âš™ï¸ Training Configuration

| Setting           | Value               |
|------------------|---------------------|
| Optimizer        | Adam                |
| Loss Function    | CrossEntropyLoss     |
| Learning Rate    | 0.0005              |
| Weight Decay     | 1e-4                |
| Batch Size       | 128                 |
| Epochs           | 10 (configurable)   |
| Scheduler        | StepLR (gamma=0.5)  |

Training and test accuracy are evaluated per epoch.

---

## ğŸ“‰ Visualizations

### ğŸ”º Accuracy Plot
- Compares **training vs. test accuracy** across epochs.

### ğŸ“‰ Smoothed Loss Curve
- Shows **loss vs. iterations** with smoothing using moving average.
- Log-scaled Y-axis to visualize exponential drop.

<img src="loss_plot_example.png" alt="Loss vs. Iterations" width="600">

---

## ğŸš€ How to Run

1. **Clone this repo**
   ```bash
   git clone https://github.com/your-username/mnist-cnn-classifier.git
   cd mnist-cnn-classifier
   ```

2. **Install dependencies**
   ```bash
   pip install torch torchvision scikit-learn matplotlib keras
   ```

3. **Run the training script**
   ```bash
   python train.py
   ```

4. **Output**
   - Accuracy plots
   - Loss curve
   - Sample predictions

---

## ğŸ“¦ Sample Prediction Output

10 random test samples with model predictions:

```
True: 7 | Predicted: 7
True: 2 | Predicted: 2
True: 1 | Predicted: 1
...
```

![Sample Predictions](sample_predictions.png)

---

## ğŸ“˜ Learnings & Takeaways

This MNIST digit classification project using a custom Convolutional Neural Network (CNN) revealed several important insights about model design, training, and evaluation:

### 1. ğŸ–¼ï¸ Image Shape Matters
MNIST images are grayscale, requiring a single channel (1x28x28).

### 2. ğŸ§  Model Depth Helps
Stacking 3 convolutional layers followed by ReLU, pooling, and dropout provided strong performance. Deeper CNNs can extract more robust spatial hierarchies.

### 3. ğŸ“Š Log-scale Loss Graphs
Plotting training loss on a logarithmic scale highlights the exponential decrease of loss, making convergence behavior easier to analyze.

### 4. ğŸ§ª Training vs Test Accuracy
Tracking both train and test accuracy per epoch helps detect overfitting. A rising and close match between them indicates good generalization.

### 5. ğŸ“‰ Learning Rate Scheduling
Using a `StepLR` scheduler reduces learning rate after a few epochs.

### 6. ğŸ” Visual Prediction Analysis
Displaying 10 random test images with predicted and true labels is a quick and intuitive way to check model behavior and catch edge cases.

### 7. ğŸ“ F1 Score for Robust Evaluation
Beyond accuracy, weighted F1-score gives a better reflection of overall performance, especially helpful when dealing with class imbalance.

---


## ğŸ‘¨â€ğŸ’» Author

- Name: *Your Name*
- GitHub: [@your-username](https://github.com/your-username)
- LinkedIn / Blog / Portfolio (optional)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
